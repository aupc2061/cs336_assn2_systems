{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3676a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n"
     ]
    }
   ],
   "source": [
    "s = \"hello! こんにちは!\"\n",
    "enc = s.encode(\"utf-8\")\n",
    "enc_s = list(enc)\n",
    "print(enc_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5b9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad0d710",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello! こんにちは!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[1;34m(bytestring)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61306f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3de93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', \"'\", 'is', ' is', ' good', ' for', ' him', ' to', ' it', \"'s\", ' very', ' as', '.']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "print(re.findall(PAT, \"t'is is good for him to it's very as.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a138271",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\"\n",
    "# es = re.findall(PAT, text)\n",
    "# es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ffbc78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "enc : dict[tuple[bytes], int] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "205dc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encs = [tuple(k.group(0).encode(\"utf-8\")) for k in re.finditer(PAT, text)]\n",
    "encs = [tuple(bytes([b]) for b in k.encode(\"utf-8\")) for k in text.split(\" \")]\n",
    "# encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "acde66f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'l', b'o', b'w'): 5,\n",
       " (b'l', b'o', b'w', b'e', b'r'): 2,\n",
       " (b'w', b'i', b'd', b'e', b's', b't'): 3,\n",
       " (b'n', b'e', b'w', b'e', b's', b't'): 6}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for e in encs:\n",
    "    enc[e] = enc.get(e, 0) + 1\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45630f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for e in enc:\n",
    "    for i, j in zip(e[:-1], e[1:]):\n",
    "        d[(i, j)] = d.get((i, j), 0) + enc[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3e478dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b's', b't')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = max(d, key=lambda x: (d[x], x))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3434d79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'l', b'o', b'w'): 5,\n",
       " (b'l', b'o', b'w', b'e', b'r'): 2,\n",
       " (b'w', b'i', b'd', b'e', b'st'): 3,\n",
       " (b'n', b'e', b'w', b'e', b'st'): 6}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build new encoding now\n",
    "new_enc = {}\n",
    "merges = [] \n",
    "merges.append(s)\n",
    "for tok, freq in enc.items():\n",
    "    new_tok = []\n",
    "    i = 0\n",
    "    while i < len(tok):\n",
    "        if i < len(tok) - 1 and (tok[i], tok[i + 1]) == s:\n",
    "            new_tok.append(tok[i] + tok[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tok.append(tok[i])\n",
    "            i += 1\n",
    "    new_enc[tuple(new_tok)] = new_enc.get(tuple(new_tok), 0) + freq\n",
    "new_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "456d59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hello, world! I'm testing GPT-2's BPE tokenizer.\n",
    "Don't split wrongly: numbers like 123, punctuation!!! or spaces   matter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6053bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# encs = [tuple(bytes([b]) for b in k.encode(\"utf-8\")) for k in text.split(\" \")]\n",
    "encs = [tuple(bytes([b]) for b in k.group(0).encode(\"utf-8\")) for k in re.finditer(PAT, text)]\n",
    "enc = {}\n",
    "for e in encs:\n",
    "    enc[e] = enc.get(e, 0) + 1\n",
    "num_merges = 6\n",
    "for _ in range(num_merges):\n",
    "    d = {}\n",
    "    for e in enc:\n",
    "        for i, j in zip(e[:-1], e[1:]):\n",
    "            d[(i, j)] = d.get((i, j), 0) + enc[e]\n",
    "    if not d:\n",
    "        break\n",
    "    s = max(d, key=lambda x: (d[x], x))\n",
    "    # s = max(d, key = d.get)\n",
    "    merges.append(s)\n",
    "    new_enc = {}\n",
    "    for tok, freq in enc.items():\n",
    "        new_tok = []\n",
    "        i = 0\n",
    "        while i < len(tok):\n",
    "            if i < len(tok) - 1 and (tok[i], tok[i + 1]) == s:\n",
    "                new_tok.append(tok[i] + tok[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tok.append(tok[i])\n",
    "                i += 1\n",
    "        new_enc[tuple(new_tok)] = new_enc.get(tuple(new_tok), 0) + freq\n",
    "    enc = new_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "66f4e224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'H', b'e', b'l', b'l', b'o'): 1,\n",
       " (b',',): 2,\n",
       " (b' ', b'w', b'or', b'l', b'd'): 1,\n",
       " (b'!',): 1,\n",
       " (b' ', b'I'): 1,\n",
       " (b\"'\", b'm'): 1,\n",
       " (b' ', b't', b'e', b's', b'ti', b'n', b'g'): 1,\n",
       " (b' ', b'G', b'P', b'T'): 1,\n",
       " (b'-',): 1,\n",
       " (b'2',): 1,\n",
       " (b\"'\", b's'): 1,\n",
       " (b' ', b'B', b'P', b'E'): 1,\n",
       " (b' ', b't', b'o', b'k', b'e', b'n', b'i', b'z', b'er'): 1,\n",
       " (b'.',): 2,\n",
       " (b'\\n',): 2,\n",
       " (b'D', b'on'): 1,\n",
       " (b\"'\", b't'): 1,\n",
       " (b' ', b'sp', b'li', b't'): 1,\n",
       " (b' ', b'w', b'r', b'on', b'g', b'l', b'y'): 1,\n",
       " (b':',): 1,\n",
       " (b' ', b'n', b'u', b'm', b'b', b'er', b's'): 1,\n",
       " (b' ', b'li', b'k', b'e'): 1,\n",
       " (b' ', b'1', b'2', b'3'): 1,\n",
       " (b' ', b'p', b'u', b'n', b'c', b't', b'u', b'a', b'ti', b'on'): 1,\n",
       " (b'!', b'!', b'!'): 1,\n",
       " (b' ', b'or'): 1,\n",
       " (b' ', b'sp', b'a', b'c', b'e', b's'): 1,\n",
       " (b' ', b' '): 1,\n",
       " (b' ', b'm', b'a', b't', b't', b'er'): 1}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4a8c58db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'o', b'n'),\n",
       " (b'e', b'r'),\n",
       " (b't', b'i'),\n",
       " (b's', b'p'),\n",
       " (b'o', b'r'),\n",
       " (b'l', b'i')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fec6780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pretokenized_counts.pkl\", \"rb\") as f:\n",
    "    fp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b6a852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {}\n",
    "for tok, freq in fp.items():\n",
    "    bt = tuple(bytes([b]) for b in tok.encode(\"utf-8\"))\n",
    "    enc[bt] = enc.get(bt, 0) + freq\n",
    "num_merges = 256\n",
    "merges = []\n",
    "d = {}\n",
    "for e in enc:\n",
    "    for i, j in zip(e[:-1], e[1:]):\n",
    "        d[(i, j)] = d.get((i, j), 0) + enc[e]\n",
    "for _ in range(num_merges):\n",
    "    if not d:\n",
    "        break\n",
    "    s = max(d, key=lambda x: (d[x], x))\n",
    "    # s = max(d, key = d.get)\n",
    "    del d[s]\n",
    "    merges.append(s)\n",
    "    new_enc = {}\n",
    "    for tok, freq in enc.items():\n",
    "        new_tok = []\n",
    "        i = 0\n",
    "        while i < len(tok):\n",
    "            if i < len(tok) - 1 and (tok[i], tok[i + 1]) == s:\n",
    "                merged = tok[i] + tok[i + 1]\n",
    "                if new_tok: # left neighbour, so decrement count for that and increase for new merged\n",
    "                    old_leftn = (new_tok[-1], tok[i])\n",
    "                    d[old_leftn] = d.get(old_leftn, 0) - freq\n",
    "                    if d[old_leftn] <= 0:\n",
    "                        d.pop(old_leftn, None)\n",
    "                    new_ln = (new_tok[-1], merged)\n",
    "                    d[new_ln] = d.get(new_ln, 0) + freq\n",
    "                if i + 2 < len(tok): # right neighbour, so decrement count for that and increase for new merged\n",
    "                    old_rightn = (tok[i + 1], tok[i + 2])\n",
    "                    d[old_rightn] = d.get(old_rightn, 0) - freq\n",
    "                    if d[old_rightn] <= 0:\n",
    "                        d.pop(old_rightn, None)\n",
    "                    new_rn = (merged, tok[i + 2])\n",
    "                    d[new_rn] = d.get(new_rn, 0) + freq\n",
    "                new_tok.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tok.append(tok[i])\n",
    "                i += 1\n",
    "        new_enc[tuple(new_tok)] = new_enc.get(tuple(new_tok), 0) + freq\n",
    "    enc = new_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d8a397f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {bytes([i]) : i for i in range(256)}\n",
    "for i, (a, b) in enumerate(merges):\n",
    "    vocab[a + b] = i + 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09704d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(filepath: str, vocab_size: int = 512, special_tokens: list[str] = None) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Train a BPE tokenizer on a given text file.\n",
    "       Inputs:\n",
    "           filepath: path to the text file to train on\n",
    "           vocab_size: desired vocabulary size (including initial byte vocab)\n",
    "           special_tokens: list of special tokens to include in the vocabulary\n",
    "       Returns:\n",
    "           vocab: dictionary mapping token IDs to byte sequences\n",
    "           merges: list of byte pair merges performed during training\n",
    "    \"\"\"\n",
    "    # Read the file and get pretokenized counts\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        fp = pickle.load(f)\n",
    "    \n",
    "    # Initialize encoding with byte pairs\n",
    "    enc = {}\n",
    "    for tok, freq in fp.items():\n",
    "        bt = tuple(bytes([b]) for b in tok.encode(\"utf-8\"))\n",
    "        enc[bt] = enc.get(bt, 0) + freq\n",
    "    \n",
    "    # Perform BPE merges\n",
    "    merges = []\n",
    "    d = {}\n",
    "    for e in enc:\n",
    "        for i, j in zip(e[:-1], e[1:]):\n",
    "            d[(i, j)] = d.get((i, j), 0) + enc[e]\n",
    "    \n",
    "    num_merges = vocab_size - 256  # Subtract initial byte vocabulary\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        if not d:\n",
    "            break\n",
    "        s = max(d, key=lambda x: (d[x], x))\n",
    "        del d[s]\n",
    "        merges.append(s)\n",
    "        \n",
    "        new_enc = {}\n",
    "        for tok, freq in enc.items():\n",
    "            new_tok = []\n",
    "            i = 0\n",
    "            while i < len(tok):\n",
    "                if i < len(tok) - 1 and (tok[i], tok[i + 1]) == s:\n",
    "                    merged = tok[i] + tok[i + 1]\n",
    "\n",
    "                    if new_tok: # left neighbour, so decrement count for that and increase for new merged\n",
    "                        old_leftn = (new_tok[-1], tok[i])\n",
    "                        d[old_leftn] = d.get(old_leftn, 0) - freq\n",
    "                        if d[old_leftn] <= 0:\n",
    "                            d.pop(old_leftn, None)\n",
    "                        new_ln = (new_tok[-1], merged)\n",
    "                        d[new_ln] = d.get(new_ln, 0) + freq\n",
    "\n",
    "                    if i + 2 < len(tok): # right neighbour, so decrement count for that and increase for new merged\n",
    "                        old_rightn = (tok[i + 1], tok[i + 2])\n",
    "                        d[old_rightn] = d.get(old_rightn, 0) - freq\n",
    "                        if d[old_rightn] <= 0:\n",
    "                            d.pop(old_rightn, None)\n",
    "                        new_rn = (merged, tok[i + 2])\n",
    "                        d[new_rn] = d.get(new_rn, 0) + freq\n",
    "                    new_tok.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tok.append(tok[i])\n",
    "                    i += 1\n",
    "            new_enc[tuple(new_tok)] = new_enc.get(tuple(new_tok), 0) + freq\n",
    "        enc = new_enc\n",
    "        \n",
    "    # Build vocabulary\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    for i, (a, b) in enumerate(merges):\n",
    "        vocab[i + 256] = a + b\n",
    "    \n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f594039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bpe_vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open(\"bpe_merges.pkl\", \"rb\") as f:\n",
    "    merges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f509b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>',\n",
       " 257: b' t',\n",
       " 258: b'he',\n",
       " 259: b' a',\n",
       " 260: b' s',\n",
       " 261: b' w',\n",
       " 262: b' the',\n",
       " 263: b'nd',\n",
       " 264: b'ed',\n",
       " 265: b' b',\n",
       " 266: b' to',\n",
       " 267: b' and',\n",
       " 268: b' h',\n",
       " 269: b' f',\n",
       " 270: b' T',\n",
       " 271: b'in',\n",
       " 272: b' wa',\n",
       " 273: b're',\n",
       " 274: b'it',\n",
       " 275: b'ou',\n",
       " 276: b' l',\n",
       " 277: b' d',\n",
       " 278: b' c',\n",
       " 279: b' p',\n",
       " 280: b'ay',\n",
       " 281: b' m',\n",
       " 282: b'er',\n",
       " 283: b' was',\n",
       " 284: b' The',\n",
       " 285: b'om',\n",
       " 286: b' he',\n",
       " 287: b'is',\n",
       " 288: b' n',\n",
       " 289: b'im',\n",
       " 290: b'ar',\n",
       " 291: b'on',\n",
       " 292: b' sa',\n",
       " 293: b'll',\n",
       " 294: b'id',\n",
       " 295: b' ha',\n",
       " 296: b' g',\n",
       " 297: b'at',\n",
       " 298: b' S',\n",
       " 299: b'ing',\n",
       " 300: b'ot',\n",
       " 301: b'en',\n",
       " 302: b'an',\n",
       " 303: b'le',\n",
       " 304: b'or',\n",
       " 305: b'ir',\n",
       " 306: b'am',\n",
       " 307: b'et',\n",
       " 308: b' H',\n",
       " 309: b' it',\n",
       " 310: b' th',\n",
       " 311: b'ig',\n",
       " 312: b' They',\n",
       " 313: b' pl',\n",
       " 314: b' in',\n",
       " 315: b'il',\n",
       " 316: b' He',\n",
       " 317: b' \"',\n",
       " 318: b'ow',\n",
       " 319: b'ver',\n",
       " 320: b'ri',\n",
       " 321: b' u',\n",
       " 322: b'ut',\n",
       " 323: b' play',\n",
       " 324: b'ith',\n",
       " 325: b' said',\n",
       " 326: b' be',\n",
       " 327: b' day',\n",
       " 328: b' with',\n",
       " 329: b'pp',\n",
       " 330: b'On',\n",
       " 331: b' o',\n",
       " 332: b' y',\n",
       " 333: b'oo',\n",
       " 334: b'ked',\n",
       " 335: b' r',\n",
       " 336: b' her',\n",
       " 337: b'ce',\n",
       " 338: b'ld',\n",
       " 339: b' his',\n",
       " 340: b' Tim',\n",
       " 341: b' I',\n",
       " 342: b' She',\n",
       " 343: b' st',\n",
       " 344: b'ke',\n",
       " 345: b' e',\n",
       " 346: b' big',\n",
       " 347: b'nt',\n",
       " 348: b'ck',\n",
       " 349: b'very',\n",
       " 350: b' you',\n",
       " 351: b'st',\n",
       " 352: b've',\n",
       " 353: b'end',\n",
       " 354: b'un',\n",
       " 355: b' happ',\n",
       " 356: b' on',\n",
       " 357: b'all',\n",
       " 358: b'riend',\n",
       " 359: b' friend',\n",
       " 360: b' they',\n",
       " 361: b' L',\n",
       " 362: b'ily',\n",
       " 363: b' we',\n",
       " 364: b' had',\n",
       " 365: b' up',\n",
       " 366: b' li',\n",
       " 367: b' not',\n",
       " 368: b'her',\n",
       " 369: b' want',\n",
       " 370: b'itt',\n",
       " 371: b' of',\n",
       " 372: b'ad',\n",
       " 373: b' do',\n",
       " 374: b' B',\n",
       " 375: b'se',\n",
       " 376: b' happy',\n",
       " 377: b'ent',\n",
       " 378: b' very',\n",
       " 379: b' M',\n",
       " 380: b'es',\n",
       " 381: b' saw',\n",
       " 382: b'One',\n",
       " 383: b' that',\n",
       " 384: b'ould',\n",
       " 385: b\"'s\",\n",
       " 386: b' for',\n",
       " 387: b'ittle',\n",
       " 388: b' mom',\n",
       " 389: b' little',\n",
       " 390: b' so',\n",
       " 391: b' sh',\n",
       " 392: b' she',\n",
       " 393: b'ime',\n",
       " 394: b'ch',\n",
       " 395: b' nam',\n",
       " 396: b' time',\n",
       " 397: b' k',\n",
       " 398: b' ne',\n",
       " 399: b'ound',\n",
       " 400: b'.\"',\n",
       " 401: b' there',\n",
       " 402: b' named',\n",
       " 403: b' bo',\n",
       " 404: b' sm',\n",
       " 405: b' were',\n",
       " 406: b' wanted',\n",
       " 407: b' Lily',\n",
       " 408: b' friends',\n",
       " 409: b'out',\n",
       " 410: b'ird',\n",
       " 411: b' but',\n",
       " 412: b'ved',\n",
       " 413: b'ht',\n",
       " 414: b'!\"',\n",
       " 415: b'The',\n",
       " 416: b' Tom',\n",
       " 417: b' bird',\n",
       " 418: b'el',\n",
       " 419: b'ake',\n",
       " 420: b' an',\n",
       " 421: b'al',\n",
       " 422: b' too',\n",
       " 423: b'ome',\n",
       " 424: b' went',\n",
       " 425: b' wh',\n",
       " 426: b'ide',\n",
       " 427: b'Once',\n",
       " 428: b' all',\n",
       " 429: b' It',\n",
       " 430: b' hel',\n",
       " 431: b'ug',\n",
       " 432: b'ue',\n",
       " 433: b' help',\n",
       " 434: b' is',\n",
       " 435: b' loo',\n",
       " 436: b' A',\n",
       " 437: b' upon',\n",
       " 438: b' lo',\n",
       " 439: b'ter',\n",
       " 440: b'ry',\n",
       " 441: b' toy',\n",
       " 442: b'ore',\n",
       " 443: b' fun',\n",
       " 444: b'ind',\n",
       " 445: b'get',\n",
       " 446: b'ill',\n",
       " 447: b'ame',\n",
       " 448: b' as',\n",
       " 449: b' j',\n",
       " 450: b'ra',\n",
       " 451: b' at',\n",
       " 452: b'gether',\n",
       " 453: b' cat',\n",
       " 454: b' did',\n",
       " 455: b' re',\n",
       " 456: b'ur',\n",
       " 457: b' together',\n",
       " 458: b'ack',\n",
       " 459: b' se',\n",
       " 460: b'ly',\n",
       " 461: b' tre',\n",
       " 462: b'ood',\n",
       " 463: b' dog',\n",
       " 464: b'ic',\n",
       " 465: b' could',\n",
       " 466: b'ted',\n",
       " 467: b' can',\n",
       " 468: b' ball',\n",
       " 469: b'ard',\n",
       " 470: b' gir',\n",
       " 471: b' their',\n",
       " 472: b'ark',\n",
       " 473: b' girl',\n",
       " 474: b' played',\n",
       " 475: b'ec',\n",
       " 476: b'my',\n",
       " 477: b' him',\n",
       " 478: b' go',\n",
       " 479: b'way',\n",
       " 480: b' ro',\n",
       " 481: b'?\"',\n",
       " 482: b'hed',\n",
       " 483: b'ain',\n",
       " 484: b' kn',\n",
       " 485: b' le',\n",
       " 486: b' out',\n",
       " 487: b'hen',\n",
       " 488: b' are',\n",
       " 489: b' fr',\n",
       " 490: b'um',\n",
       " 491: b\"'t\",\n",
       " 492: b' them',\n",
       " 493: b' boy',\n",
       " 494: b' sad',\n",
       " 495: b'ul',\n",
       " 496: b'ax',\n",
       " 497: b' tree',\n",
       " 498: b'other',\n",
       " 499: b'oug',\n",
       " 500: b' man',\n",
       " 501: b' loved',\n",
       " 502: b' have',\n",
       " 503: b' cl',\n",
       " 504: b' found',\n",
       " 505: b' looked',\n",
       " 506: b' sp',\n",
       " 507: b' Sue',\n",
       " 508: b' star',\n",
       " 509: b' sc',\n",
       " 510: b'one',\n",
       " 511: b' back'}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c7c01fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the cat ate\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "encs = [tuple(bytes([b]) for b in k.group(0).encode(\"utf-8\")) for k in re.finditer(PAT, text)]\n",
    "merge_ranks = {p: i for i, p in enumerate(merges)}\n",
    "for i, enc in enumerate(encs):\n",
    "    while True:\n",
    "        pairs = [(enc[j], enc[j + 1]) for j in range(len(enc) - 1)]\n",
    "        rankp = [(merge_ranks[p], j, p) for j, p in enumerate(pairs) if p in merge_ranks]\n",
    "        if not rankp:\n",
    "            break\n",
    "        _, j, _ = min(rankp)\n",
    "        enc = enc[:j] + (enc[j] + enc[j + 1],) + enc[j + 2:]\n",
    "    encs[i] = enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d9dddc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2id = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5f971bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116, 258, 453, 451, 101]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = []\n",
    "for i in range(len(encs)):\n",
    "    encoded.extend([tok2id[x] for x in encs[i]])\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dc7a85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoded: list, vocab: dict[int, bytes]):\n",
    "    s = b\"\".join([vocab[k] for k in encoded]).decode(\"utf-8\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4765aa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat ate'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encoded, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "190ed7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.tok2id = {v: k for k, v in self.vocab.items()}\n",
    "        self.merge_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        encs = [tuple(bytes([b]) for b in k.group(0).encode(\"utf-8\")) for k in re.finditer(PAT, text)]\n",
    "        for i, enc in enumerate(encs):\n",
    "            while True:\n",
    "                pairs = [(enc[j], enc[j + 1]) for j in range(len(enc) - 1)]\n",
    "                rankp = [(self.merge_ranks[p], j, p) for j, p in enumerate(pairs) if p in self.merge_ranks]\n",
    "                if not rankp:\n",
    "                    break\n",
    "                _, j, _ = min(rankp)\n",
    "                enc = enc[:j] + (enc[j] + enc[j + 1],) + enc[j + 2:]\n",
    "            encs[i] = enc\n",
    "        encoded = []\n",
    "        for i in range(len(encs)):\n",
    "            encoded.extend([self.tok2id[x] for x in encs[i]])\n",
    "        return encoded\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None):\n",
    "        with open(vocab_filepath, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        \n",
    "        with open(merges_filepath, \"rb\") as f:\n",
    "            merges = pickle.load(f)\n",
    "\n",
    "        return cls(vocab=vocab, merges=merges, special_tokens=special_tokens)\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "       s = b\"\".join([self.vocab[k] for k in ids]).decode(\"utf-8\")\n",
    "       return s \n",
    "    \n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for s in iterable:\n",
    "            enc = self.encode(s)\n",
    "            for i in enc:\n",
    "                yield i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06930647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import einsum\n",
    "import math\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=\"cpu\", dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else \"cpu\"\n",
    "        self.W = nn.Parameter(torch.randn((out_features, in_features), dtype=dtype, device=device))\n",
    "        sigma = math.sqrt(2 / (in_features + out_features))\n",
    "        nn.init.trunc_normal_(self.W, mean=0, std=sigma, a=-3 * sigma, b=3 * sigma)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = einsum(x, self.W, \"... i, o i -> ... o\")\n",
    "        return out\n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Parameter(torch.randn((num_embeddings, embedding_dim), device=device, dtype=dtype))\n",
    "        nn.init.trunc_normal_(self.emb, mean=0, std=1, a=-3, b=3)\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.emb[token_ids]\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.d_model = d_model\n",
    "        self.gamma = nn.Parameter(torch.eye(d_model, dtype=dtype, device=device))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (x @ self.gamma) / rms\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61afa5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.9.0+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchdata, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4b08c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cs336_basics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3518442021.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcs336_basics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs336_basics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs336_basics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs336_basics'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.train import *\n",
    "from cs336_basics.layers import TransformerLM\n",
    "from cs336_basics.tokenizer import Tokenizer\n",
    "import torch\n",
    "\n",
    "# vocab_path=r\"D:\\CS336\\assignment1-basics\\cs336_basics\\bpe_vocab.pkl\"\n",
    "# merges_path=r\"D:\\CS336\\assignment1-basics\\cs336_basics\\bpe_merges.pkl\"\n",
    "# tokenizer = Tokenizer.from_files(vocab_filepath=vocab_path, merges_filepath=merges_path)\n",
    "# device = \"cpu\"\n",
    "# model = TransformerLM(\n",
    "#         vocab_size=tokenizer.vocab_size, \n",
    "#         context_length=128, \n",
    "#         num_layers=6,\n",
    "#         d_model=256,\n",
    "#         d_ff=1024,\n",
    "#         num_heads=8,\n",
    "#         theta=10000.0).to(device=device)\n",
    "# # model = torch.compile(model)\n",
    "# model_path = r\"D:\\CS336\\assignment1-basics\\ckpt_final.pt\"\n",
    "# d = torch.load(model_path)\n",
    "# model.load_state_dict(d[\"model\"])\n",
    "\n",
    "# prompt = \"What are you doing?\"\n",
    "# print(\"Prompt length:\", len(tokenizer.encode(prompt)))\n",
    "# output = generate(model, tokenizer, prompt, 128)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df5296f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6556928"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameter tensors:\n",
    "# len(list(model.parameters()))\n",
    "\n",
    "# Total number of scalar parameters:\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a635de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy birthday they thinys it of sandglve soitmot from up! Tinow ligh up on their fun, unts Lily for comdar that another wind mom delicse beet shy wore asle gifGark from and as lat\u0019 mom shoot pill under on them!\" Ar went goit from them under The wainan uninor.\" Tim ble happy hot napl of j\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Happy birthday\"\n",
    "output = generate(model, tokenizer, prompt, 128, temperature=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab25e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>',\n",
       " 257: b' t',\n",
       " 258: b'he',\n",
       " 259: b' a',\n",
       " 260: b' s',\n",
       " 261: b' w',\n",
       " 262: b' the',\n",
       " 263: b'nd',\n",
       " 264: b'ed',\n",
       " 265: b' b',\n",
       " 266: b' to',\n",
       " 267: b' and',\n",
       " 268: b' h',\n",
       " 269: b' f',\n",
       " 270: b' T',\n",
       " 271: b'in',\n",
       " 272: b' wa',\n",
       " 273: b're',\n",
       " 274: b'it',\n",
       " 275: b'ou',\n",
       " 276: b' l',\n",
       " 277: b' d',\n",
       " 278: b' c',\n",
       " 279: b' p',\n",
       " 280: b'ay',\n",
       " 281: b' m',\n",
       " 282: b'er',\n",
       " 283: b' was',\n",
       " 284: b' The',\n",
       " 285: b'om',\n",
       " 286: b' he',\n",
       " 287: b'is',\n",
       " 288: b' n',\n",
       " 289: b'im',\n",
       " 290: b'ar',\n",
       " 291: b'on',\n",
       " 292: b' sa',\n",
       " 293: b'll',\n",
       " 294: b'id',\n",
       " 295: b' ha',\n",
       " 296: b' g',\n",
       " 297: b'at',\n",
       " 298: b' S',\n",
       " 299: b'ing',\n",
       " 300: b'ot',\n",
       " 301: b'en',\n",
       " 302: b'an',\n",
       " 303: b'le',\n",
       " 304: b'or',\n",
       " 305: b'ir',\n",
       " 306: b'am',\n",
       " 307: b'et',\n",
       " 308: b' H',\n",
       " 309: b' it',\n",
       " 310: b' th',\n",
       " 311: b'ig',\n",
       " 312: b' They',\n",
       " 313: b' pl',\n",
       " 314: b' in',\n",
       " 315: b'il',\n",
       " 316: b' He',\n",
       " 317: b' \"',\n",
       " 318: b'ow',\n",
       " 319: b'ver',\n",
       " 320: b'ri',\n",
       " 321: b' u',\n",
       " 322: b'ut',\n",
       " 323: b' play',\n",
       " 324: b'ith',\n",
       " 325: b' said',\n",
       " 326: b' be',\n",
       " 327: b' day',\n",
       " 328: b' with',\n",
       " 329: b'pp',\n",
       " 330: b'On',\n",
       " 331: b' o',\n",
       " 332: b' y',\n",
       " 333: b'oo',\n",
       " 334: b'ked',\n",
       " 335: b' r',\n",
       " 336: b' her',\n",
       " 337: b'ce',\n",
       " 338: b'ld',\n",
       " 339: b' his',\n",
       " 340: b' Tim',\n",
       " 341: b' I',\n",
       " 342: b' She',\n",
       " 343: b' st',\n",
       " 344: b'ke',\n",
       " 345: b' e',\n",
       " 346: b' big',\n",
       " 347: b'nt',\n",
       " 348: b'ck',\n",
       " 349: b'very',\n",
       " 350: b' you',\n",
       " 351: b'st',\n",
       " 352: b've',\n",
       " 353: b'end',\n",
       " 354: b'un',\n",
       " 355: b' happ',\n",
       " 356: b' on',\n",
       " 357: b'all',\n",
       " 358: b'riend',\n",
       " 359: b' friend',\n",
       " 360: b' they',\n",
       " 361: b' L',\n",
       " 362: b'ily',\n",
       " 363: b' we',\n",
       " 364: b' had',\n",
       " 365: b' up',\n",
       " 366: b' li',\n",
       " 367: b' not',\n",
       " 368: b'her',\n",
       " 369: b' want',\n",
       " 370: b'itt',\n",
       " 371: b' of',\n",
       " 372: b'ad',\n",
       " 373: b' do',\n",
       " 374: b' B',\n",
       " 375: b'se',\n",
       " 376: b' happy',\n",
       " 377: b'ent',\n",
       " 378: b' very',\n",
       " 379: b' M',\n",
       " 380: b'es',\n",
       " 381: b' saw',\n",
       " 382: b'One',\n",
       " 383: b' that',\n",
       " 384: b'ould',\n",
       " 385: b\"'s\",\n",
       " 386: b' for',\n",
       " 387: b'ittle',\n",
       " 388: b' mom',\n",
       " 389: b' little',\n",
       " 390: b' so',\n",
       " 391: b' sh',\n",
       " 392: b' she',\n",
       " 393: b'ime',\n",
       " 394: b'ch',\n",
       " 395: b' nam',\n",
       " 396: b' time',\n",
       " 397: b' k',\n",
       " 398: b' ne',\n",
       " 399: b'ound',\n",
       " 400: b'.\"',\n",
       " 401: b' there',\n",
       " 402: b' named',\n",
       " 403: b' bo',\n",
       " 404: b' sm',\n",
       " 405: b' were',\n",
       " 406: b' wanted',\n",
       " 407: b' Lily',\n",
       " 408: b' friends',\n",
       " 409: b'out',\n",
       " 410: b'ird',\n",
       " 411: b' but',\n",
       " 412: b'ved',\n",
       " 413: b'ht',\n",
       " 414: b'!\"',\n",
       " 415: b'The',\n",
       " 416: b' Tom',\n",
       " 417: b' bird',\n",
       " 418: b'el',\n",
       " 419: b'ake',\n",
       " 420: b' an',\n",
       " 421: b'al',\n",
       " 422: b' too',\n",
       " 423: b'ome',\n",
       " 424: b' went',\n",
       " 425: b' wh',\n",
       " 426: b'ide',\n",
       " 427: b'Once',\n",
       " 428: b' all',\n",
       " 429: b' It',\n",
       " 430: b' hel',\n",
       " 431: b'ug',\n",
       " 432: b'ue',\n",
       " 433: b' help',\n",
       " 434: b' is',\n",
       " 435: b' loo',\n",
       " 436: b' A',\n",
       " 437: b' upon',\n",
       " 438: b' lo',\n",
       " 439: b'ter',\n",
       " 440: b'ry',\n",
       " 441: b' toy',\n",
       " 442: b'ore',\n",
       " 443: b' fun',\n",
       " 444: b'ind',\n",
       " 445: b'get',\n",
       " 446: b'ill',\n",
       " 447: b'ame',\n",
       " 448: b' as',\n",
       " 449: b' j',\n",
       " 450: b'ra',\n",
       " 451: b' at',\n",
       " 452: b'gether',\n",
       " 453: b' cat',\n",
       " 454: b' did',\n",
       " 455: b' re',\n",
       " 456: b'ur',\n",
       " 457: b' together',\n",
       " 458: b'ack',\n",
       " 459: b' se',\n",
       " 460: b'ly',\n",
       " 461: b' tre',\n",
       " 462: b'ood',\n",
       " 463: b' dog',\n",
       " 464: b'ic',\n",
       " 465: b' could',\n",
       " 466: b'ted',\n",
       " 467: b' can',\n",
       " 468: b' ball',\n",
       " 469: b'ard',\n",
       " 470: b' gir',\n",
       " 471: b' their',\n",
       " 472: b'ark',\n",
       " 473: b' girl',\n",
       " 474: b' played',\n",
       " 475: b'ec',\n",
       " 476: b'my',\n",
       " 477: b' him',\n",
       " 478: b' go',\n",
       " 479: b'way',\n",
       " 480: b' ro',\n",
       " 481: b'?\"',\n",
       " 482: b'hed',\n",
       " 483: b'ain',\n",
       " 484: b' kn',\n",
       " 485: b' le',\n",
       " 486: b' out',\n",
       " 487: b'hen',\n",
       " 488: b' are',\n",
       " 489: b' fr',\n",
       " 490: b'um',\n",
       " 491: b\"'t\",\n",
       " 492: b' them',\n",
       " 493: b' boy',\n",
       " 494: b' sad',\n",
       " 495: b'ul',\n",
       " 496: b'ax',\n",
       " 497: b' tree',\n",
       " 498: b'other',\n",
       " 499: b'oug',\n",
       " 500: b' man',\n",
       " 501: b' loved',\n",
       " 502: b' have',\n",
       " 503: b' cl',\n",
       " 504: b' found',\n",
       " 505: b' looked',\n",
       " 506: b' sp',\n",
       " 507: b' Sue',\n",
       " 508: b' star',\n",
       " 509: b' sc',\n",
       " 510: b'one',\n",
       " 511: b' back'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d50b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e89e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 30 13:08:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
